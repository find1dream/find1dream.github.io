<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://find1dream.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://find1dream.github.io/" rel="alternate" type="text/html" /><updated>2020-07-01T18:57:32+09:00</updated><id>http://find1dream.github.io/feed.xml</id><title type="html">Blog</title><subtitle>A better way to express ideas
</subtitle><author><name>Linfeng Chen</name></author><entry xml:lang="ch"><title type="html">DIY 申请高度专门职1号那些事</title><link href="http://find1dream.github.io/ch/get_highly_skilled_foreign_professionals_visa_in_japan" rel="alternate" type="text/html" title="DIY 申请高度专门职1号那些事" /><published>2020-06-19T00:00:00+09:00</published><updated>2020-06-19T00:00:00+09:00</updated><id>http://find1dream.github.io/ch/get_highly_skilled_foreign_professionals_visa_in_japan</id><content type="html" xml:base="http://find1dream.github.io/ch/get_highly_skilled_foreign_professionals_visa_in_japan">## 前言

在日本工作需要签证，然而就职签证有很多种。虽说近来日本的就职签证通过概率比较高，但是可在留时长会有很大的变化。第一次申请普通的就职签证，很可能只有一年在留期间，然而申请高度专门职 1 号签证的话，一次性 5 年签证。申请高度专门职 2 号（拥有 1 号的前提下）的话，在留期限无限制。理论上就是“永住”了。因为公司比较小，签证这些事只能自己搞。我来日本两年半，最近刚从大学院毕业，从留学签证换到高度专门职 1 号ロ，在这里记录一下经验，供有需要的人参考。

![](/assets/img/share/japan/visa.png)




## 什么是高度专门职？

&gt; 高度人材と呼ばれる優秀な外国人を日本に呼び込み、日本国内の活性化を目指すべく創設された在留資格です。外国人の学歴や職歴、年収などを点数化し、高度の専門知識や技術を持つ高度人材の受け入れの判断基準となる「高度人材ポイント制」を導入しています。

### 分类

听名字挺高大上的高度专门职，主要根据分数来判断是否下发这个签证。高度专门职分为三种：

* 高度専門職1号イ
  * 高度学術研究活動、日本の公私の機関との契約に基づいて行う研究、研究の指導または教育をする活動。主要是面向搞学术的人
* 高度専門職1号ロ
  * 高度専門・技術活動、日本の公私の機関との契約に基づいて行う自然科学または人文科学の分野に属する知識または技術を要する業務に従事する活動。主要是面向公司工作的工程师，设计师等。这个覆盖面最广，基本普通社员分数够了的话都在这个签证里
* 高度専門職1号ハ
  * 高度経営・管理活動、日本の公私の機関において事業の経営を行いまたは管理に従事する活動。覆盖投资人士，开公司的老板等

### 高度专门职 1 号和 2 号又是什么东西？

要申请高度专门职 2 号的话。

&gt;「高度専門職1号」又は高度外国人材としての「特定活動」の在留資格をもって本邦に3年以 上在留して当該在留資格に該当する活動を行っていたこと

也就是说，必须要是有 1 号签证或者特定活动签证的前提下，在日本干了三年活才可以申请 2 号。1 号和 2 号的差别的话，主要是 2 号是无限期签证，并且副业范围稍微拓展了一点。

## 为什么要申请高度专门职签证？

* 可以进行相关活动的副业。普通的签证的话只能进行那一种职业活动
  * 比如 IT 工程师，可以写技术博客，搞编程培训等 IT 相关副业，并且不需要去入国管理局得到资格外活动许可
* 优先处理
  * 10 天以内出结果。这个很给力，我只用一周就拿到了这个签证。正常的就劳签证是需要 1-3 个月
* 一律 5 年在留期间
  * 一般的就职签证是 1-3 年，一次给 5 年的话，会方便很多很多，正常有高度专门签证的人 5 年内就直接申请永住了
* 永住年数缓和
  * 通常永住需要在日本干 10 年的活才行。但如果是学生，读 1 年语言学校，4 年大学，5 年大学院，理论上还得在日本干 10 年的活（公司就职之类）才能申请永住，加起来可就 20 年了。但是有了高度专门职这个制度，最慢干 3 年，最快可以只干 1 年就申请永驻
* 配偶的就劳
  * 简单地说就是对象（非日本人的话）也可以在日本工作
* 可带父母来日本
  * 条件比较多，最好看看官网要求
* 可带仆人保姆过来
  * 有点资本主义社会的气息。

## 如何 DIY 申请高度专门职签证？

### 检查你的分数够不够

这是官网的[计算表](http://www.immi-moj.go.jp/newimmiact_3/pdf/h29_06_point-hyou.pdf)，算出自己的分数有没有大于等于 70，如果是的话，恭喜你，可以申请。另外多提一嘴，虽说 70 分就够了，但是能凑到 80 分最好，因为这样的话一年以后就可以直接申请永住签证了。

为什么要 DIY 申请？因为比较简单，几个小时就能搞定，而且被拒的几率很小，没必要花几万块钱去找代理机构。

这里我已自己的经历为例，详细说明如何申请高度専門職 1 号ロ。文科或者经营者的朋友们也可以参考，流程是一样的。

### 资料准备

全部资料都在[这里](http://www.immi-moj.go.jp/newimmiact_3/pdf/procedure/201802/03.pdf)，一个个搜集好就行。

#### 申请人准备的资料

* 在留资格认定证明书交付申请书
* 3 个月内拍的证件照一张，贴在上面的申请书上（注意头发别挡着眼睛，可以自己 PS，我只能帮你到这了）
* **不需要**返信用信纸，这个信息是太旧了吧，害得我买了没用上
* 公司的雇用契约书
* 护照和在留卡
* 分数计算表，没有的话当场他们会给你打，让你填上
* 证明你分数大于等于 70 分的资料，都是复印件就可以

#### 公司准备的资料

这里公司有几种，大公司估计都给办了，看这篇文章的人应该都是自己办的，这里以第三种为例（对照[这](http://www.moj.go.jp/nyuukokukanri/kouhou/nyuukokukanri07_00093.html)查清楚）

* 申请书
* 最近一年的职员的「給与所得の源泉徴収票などの法定調書合計票」的复印
* 公司的介绍（可以直接打印主页）
* 登记事项证明书
* 最近一年的决算文书
* 年收证明书

公司那边准备的资料，直接名字告诉事務的人就行，他们知道是什么。要注意公司那边可能需要准备很久，我当时是花了快两个月才给我搞出来。如果是学生的话，推荐在毕业之前就开始准备起来（比如 1 月份），不然后面可能会很仓促。

### 去入国管理局递交

这个可能是很磨人的一个地方，尤其是品川的东京出入国管理局，早上 8 点估计就排了上百个人了。第一次去是下午三点，他们是四点下班，我看了一眼排队的人数，直接就回去了。第二次早上 8 点到场，9 点 20 分左右进去，等了一个多小时终于轮到我，发现公司的一项资料（登记事项证明书）忘记了，然后又打道回府。第三次早上 9 点到场， 11 点 20 分左右进场，这次递交成功了。

### 一周收到结果

由于被拒的可能性极低，我也没有担心什么，一周后看了看邮箱，发现通知寄到了，让我带上 4000 块钱去取，这么贵的。一周办下来我居然感动了，以日本人的办事效率，这种事感觉是第一次这么快收到了结果。顺便提一句，永住申请需要等半年到一年，归化也是。速度对比出来了吧。

这次是下午 2 点去的，发现人比一大早要少不少！要是材料都准备好了，或者是去取在留卡的话，推荐下午去，人反而少一点。虽说过了下午 4 点，但是发签证的地方不会下班，而是会逐一发完。最后平安地拿到了签证。

## 这个签证可以做副业吗？

结论是可以，但是有一定的风险。

我这个签证的主要目的是为了一年后申请永住，然后干副业。虽说高度专门职也可以做副业，但是有两点需要注意的地方

* 范围受到限制，只能是在那个签证的相关范围内从事活动，比如我只能从事 IT 相关的副业，然而做一些其他的副业却不行
* 如果副业被公司发现，最坏的结果被开除的话，必须 6 个月内尽快找到工作，不然会被遣返。所以推荐先咨询下公司可不可以副业，然后再行动

## 总结和建议

总之这个签证还是很有魅力的，对于马上毕业的留学生，或者拥有其他工作签证的人来说，推荐先检查下自己的分数，够了的话可以直接申请的。可以考些资格证，没考到 N1 的加把油，争取年收高于 400 万，基本就能拿到了。这个签证的最大魅力在于申请永住，等我拿到永住后再写一篇经验贴分享给大家。


## 参考
- [在留資格変更許可申請書](http://www.moj.go.jp/ONLINE/IMMIGRATION/16-2-1.html)
- [高度専門職1号ロの高度専門職ポイント計算表](https://common-s.jp/eijuukoro.html)</content><author><name>Linfeng Chen</name></author><category term="分享" /><category term="人在日本" /><summary type="html">前言</summary></entry><entry xml:lang="en"><title type="html">How To Train Multiple Model In One Time With Sklearn</title><link href="http://find1dream.github.io/en/How_to_train_multiple_model_in_one_time_with_sklearn" rel="alternate" type="text/html" title="How To Train Multiple Model In One Time With Sklearn" /><published>2019-09-12T00:00:00+09:00</published><updated>2019-09-12T00:00:00+09:00</updated><id>http://find1dream.github.io/en/How_to_train_multiple_model_in_one_time_with_sklearn</id><content type="html" xml:base="http://find1dream.github.io/en/How_to_train_multiple_model_in_one_time_with_sklearn">Here is the code to show you how to check accuracy of multiple models in a pipeline.
```python
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score

data= load_iris()
Y_train = data.target
X_train = data.data[:, :2]
# Add machine learning model to list
models = []
models.append((&quot;KNC&quot;,KNeighborsClassifier()))
models.append((&quot;SVM&quot;,SVC()))
models.append((&quot;DTC&quot;,DecisionTreeClassifier()))

# train multiple classifier with Kfold
results = []
names = []
for name,model in models:
    kfold = KFold(n_splits=15, random_state=42)
    result = cross_val_score(model,X_train,Y_train, cv = kfold, scoring = &quot;accuracy&quot;)
    names.append(name)
    results.append(result)

# show the accuracy of the models
for i in range(len(names)):
    print(names[i],results[i].mean())
```



What's Kfold in the code? Please check this post: [The Easiest Introduction To Cross Validation](https://find1dream.github.io/en/The_Easiest_Introduction_To_Cross_Validation).</content><author><name>Linfeng Chen</name></author><category term="Tech" /><category term="Machine Learning" /><category term="Python" /><summary type="html">Here is the code to show you how to check accuracy of multiple models in a pipeline. ```python from sklearn.model_selection import KFold from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.model_selection import cross_val_score</summary></entry><entry xml:lang="en"><title type="html">Pandas Tricks</title><link href="http://find1dream.github.io/en/Pandas_tricks" rel="alternate" type="text/html" title="Pandas Tricks" /><published>2019-09-10T00:00:00+09:00</published><updated>2019-09-10T00:00:00+09:00</updated><id>http://find1dream.github.io/en/Pandas_tricks</id><content type="html" xml:base="http://find1dream.github.io/en/Pandas_tricks">### Create dataframe

```python
# create DataFrame 1
&gt;&gt;&gt; import pandas
&gt;&gt;&gt; df = pandas.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})
&gt;&gt;&gt; df
   x  y
0  1  4
1  2  5
2  3  6
```

### Converting a DataFrame to a Numpy Array

```python
df.values
or table presentation
df.as_matrix
```

### Iterate dataframe

```python
1.
&gt;&gt;&gt; z = []
&gt;&gt;&gt; for index, row in df.iterrows():
...     z.append(row.x + row.y)
... 
&gt;&gt;&gt; z
[5, 7, 9]

2.We could also write it using a list comprehension
z = [ row.x + row.y for index, row in df.iterrows() ]
We could then assign this list to our new column.

&gt;&gt;&gt; df['z'] = z
&gt;&gt;&gt; df
   x  y  z
0  1  4  5
1  2  5  7
2  3  6  9
```

###  Creating a new Column by Looping

```python
topping_type = []

for row in df.topping:
    if row in ['pepperoni', 'chicken', 'anchovies']:
        topping_type.append('meat')
    else:
        topping_type.append('vegetable')

df['topping_type'] = topping_type
```

### To apply a function to all columns or rows

```python
&gt;&gt;&gt; df = pandas.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})
&gt;&gt;&gt; df.apply(lambda row: row.x + row.y, axis=1)
0    5
1    7
2    9 
dtype: int64

or
&gt;&gt;&gt; df['z'] = df.apply(lambda row: row.x + row.y, axis=1)
&gt;&gt;&gt; df
   x  y  z
0  1  4  5
1  2  5  7
2  3  6  9


```

### Vectorized operations

```python
&gt;&gt;&gt; df = pandas.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})
&gt;&gt;&gt; df.x
0    1
1    2
2    3
Name: x, dtype: int64

&gt;&gt;&gt; df['z'] = df.x + df.y
&gt;&gt;&gt; df
   x  y  z
0  1  4  5
1  2  5  7
2  3  6  9
```

### Means/Std of dataframe

```python
df['average'] = df.mean(axis=1)
       salary_1  salary_2  salary_3     average
0       230       235       210  225.000000
1       345       375       385  368.333333
2       222       292       260  258.000000

or some of it
df['average_1_3'] = df[['salary_1', 'salary_3']].mean(axis=1)
   salary_1  salary_2  salary_3  average_1_3
0       230       235       210        220.0
1       345       375       385        365.0
2       222       292       260        241.0

another method
col = df.loc[: , &quot;salary_1&quot;:&quot;salary_3&quot;]
df['salary_mean'] = col.mean(axis=1)
```

### Sum of dataframe

```python
df['C'] = df.sum(axis=1,numeric_only=True)
or
df['C'] = df['A'] + df['B']
or
f['C'] = df.apply(lambda row: row['A'] + row['B'], axis=1)
or
df['C'] =  df[['A', 'B']].sum(axis=1)
or
df['sum'] = df[list(df.columns)].sum(axis=1)
or
df.assign(C = df.A + df.B,
          Diff = df.B - df.A,
          Mult = df.A * df.B)
# Out[379]: 
#    A  B   C  Diff  Mult
# 0  1  4   5     3     4
# 1  2  6   8     4    12
# 2  3  9  12     6    27
```

### Importing a csv file/ Loading Massive Datasets in Smaller Chunks

```python
df = pd.read_csv('pizza.csv')
#Need to parse dates? Just pass in the corresponding column name(s).

df = pd.read_csv('pizza.csv', parse_dates=['dates'])
#Only need a few specific columns?

df = pd.read_csv('pizza.csv', usecols=['foo', 'bar'])




chunksize = 500
chunks = []
for chunk in pd.read_csv('pizza.csv', chunksize=chunksize):
    # Do stuff...
    chunks.append(chunk)

df = pd.concat(chunks, axis=0)
```

### Exploring Data in a DataFrame

```python
df.head()       # first five rows
df.tail()       # last five rows
df.sample(5)    # random sample of rows
df.shape        # number of rows/columns in a tuple
df.describe()   # calculates measures of central tendency
df.info()       # memory footprint and datatypes
```

### Adding a New Column to a DataFrame

```python
#Need the column in a certain order? The first argument is the position of the column. This will put the column at the begining of the DataFrame.
df.insert(0, 'original_price', full_price)
```

### Select a Specific “Cell” Value

```python
df.loc[rowindex, 'A'] 
df.ix[2,'A']
df.iloc[row][&quot;A&quot;]
```

### Filtering dataframes with conditional logic

```python
filtered_data = df[(df.price &gt; 11.99) &amp; (df.topping == 'Pineapple')]
```

### Sorting a DataFrame by a Certain Column

```python
df.sort_values('price', axis=0, ascending=False)
df.sort_values(['avg',&quot;ID&quot;]) #first sort avg, and ID
```

### Set output precesion

```python
pandas.set_option('precision', 2)
```

### Apply a Function to Every Row in a Column

```python
def calculate_taxes(price):
    taxes = price * 0.12
    return taxes

df['taxes'] = df.price.apply(calculate_taxes)
```

### Add a New Column with Conditional Logic

```python
df['profitable'] = np.where(df['price']&gt;=15.00, True, False)
```

### Combining DataFrames with Concatenation

```python
pd.concat([df_1, df_2], axis=0)
#Or to concat columns horizontally:

pd.concat([df_1, df_2], axis=1)
```

### Combining DataFrames based on an Index Key

```python
#Merging in Pandas works just like SQL. If you you have two DataFrames that share a key, perhaps a pizza ‘order_id’, you can perform inner, outer, left, right joins just like you would in SQL.

merged_df = df_1.merge(df_2, how='left', on='order_id')
```

### Converting Dates to their own Day, Week, Month, Year Columns

```python
#make sure the data is in datetime format. Then use dt method to extract the data you need.

date = pd.to_datetime(df.date)
df['weekday'] = date.dt.weekday
df['year'] = date.dt.year
```

###  Finding NaNs in a DataFrame

```python
#Count the total number of NaNs present:

df.isnull().sum().sum()
#List the NaN count for each column:

df.isnull().sum()
```

### Filling NaNs or Missing Data

```python
df.topping = df.topping.fillna('Cheese')
# or we can drop any row missing data across the entire DataFrame:

df = df.dropna(axis=0)
```

### Extracting Features by Grouping Columns

```python
#Grouping columns is a great way to extract features from data. This is especially useful when you have data that can be counted or quantified in some way. For example, you might have group pizzas by topping, then calculate the mean for price in each group.

df.groupby('topping')['discount'].apply(lambda x: np.mean(x))
#or maybe you want to see the count of a certain value

df.groupby('topping')['discount'].apply(lambda x: x.count())

topping
Anchovies      3
Bell Pepper    1
Cheese         2
Olives         1
Pepperoni      3
Pineapple      2
Veggie         1
Name: discount, dtype: int64
```

### Creating Bins

```python
#Let’s say we want to create 3 separate bins for different price ranges. This is especially useful for simplifying noisy data.

bins = [0, 5, 15, 30]
names = ['Cheap', 'Normal', 'Expensive']

df['price_point'] = pd.cut(df.price, bins, labels=names)

 order_numbe	price	price_point
0	PZZA0000	12.99	Normal
1	PZZA0001	14.50	Normal
2	PZZA0002	19.99	Expensive
3	PZZA0003	20.99	Expensive
4	PZZA0004	21.99	Expensive

```

### Square sum

```python

```

### Delete unique data

```python
df_train.drop(df_train[df_train['GrLivArea']&gt;=4000].index,inplace=True)
```

### Select some columns

```python
df_train_shrinked=df_train.loc[:,['GrLivArea','YearBuilt','OverallCond','BsmtQual']]
df_x=df_train_shrinked

GrLivArea	YearBuilt	OverallCond	BsmtQual
0	1710	2003	5	Gd
1	1262	1976	8	Gd
2	1786	2001	5	Gd
3	1717	1915	5	TA
4	2198	2000	5	Gd
```

### Data to value

```python
df_x=pd.get_dummies(df_x)
```

### Column change types
```python
df.x = df.x.astype(float)
```

### MinMaxscaler by sklearn

```python
df[[&quot;x&quot;]] = scaler.fit_transform(df[[&quot;x&quot;]])
```


### Group sum
group by `mm`, and sum by `k`
```python
x = x.groupby('mm')['k'].sum().resun_index()
```


### Date make

```python
df = pd.DataFrame({'value': range(1,32,2)},
     index=pd.date_range('2019-01-01', '2019-01-31',freq='2D'))  # &quot;W&quot; week
```


### Change a column from category to data by dict

```python
dictA = {&quot;m&quot;:1,...}
df['x'] = df['x'].apply(dictA.get)
```</content><author><name>Linfeng Chen</name></author><category term="Tech" /><category term="Python" /><category term="Pandas" /><summary type="html">Create dataframe</summary></entry><entry xml:lang="en"><title type="html">A Least Squares Approach</title><link href="http://find1dream.github.io/en/A_least_squares_approach" rel="alternate" type="text/html" title="A Least Squares Approach" /><published>2019-09-08T00:00:00+09:00</published><updated>2019-09-08T00:00:00+09:00</updated><id>http://find1dream.github.io/en/A_least_squares_approach</id><content type="html" xml:base="http://find1dream.github.io/en/A_least_squares_approach">## The essentials
Choose w to get the min Loss function.

{% katexmm display %}
$$\mathcal { L } = \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \mathcal { L } _ { n } \left( t _ { n } , f \left( x _ { n } ; \vec{w} \right) \right)$$ 
$$\underset { \vec{w} } { \operatorname { argmin } } \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \mathcal { L } _ { n } \left( t _ { n } , f \left( x _ { n } ; \vec{w} \right) \right)$$
we can change $\mathcal { L } _ { n }$ to fit our needs, but here we use the mostly used square root sum function.\\
$$\mathcal { L } _ { n } \left( t _ { n } , f \left( x _ { n } ; w _ { 0 } , w _ { 1 } \right) \right) = \left( t _ { n } - f \left( x _ { n } ; w _ { 0 } , w _ { 1 } \right) \right) ^ { 2 }$$

## Turning point
Goal: to choose the minimum in a mathmatical way.
How it works: the partial derivatives of $\mathbf{w}$ should be $0$, and second derivatives should be a positive value to ensure this is a minimum/maxmum

## Example
$$\mathbf{w} =\left[ \begin{array} { l } { w _ { 0 } } \\ { w _ { 1 } } \end{array} \right] \Rightarrow \begin{cases}
 &amp; \widehat { w _ { 0 } } = \overline { t } - w _ { 1 } \overline { x }\\ 
 &amp; \widehat { w _ { 1 } } = \frac { \overline { x t } - \overline { x } \overline { t } } { x ^ { 2 } - ( \overline { x } ) ^ { 2 } }
\end{cases} $$
{% endkatexmm %}

```python
#data.csv
n xn tn xntn x2
1 1896 12.00 22752.0 3.5948*10**6
2 1900 11.00 20900.0 3.6100*10**6
3 1904 11.00 20944.0 3.6252*10**6
4 1906 11.20 21347.2 3.6328*10**6
5 1908 10.80 20606.4 3.6405*10**6
6 1912 10.80 20649.6 3.6557*10**6
7 1920 10.80 20736.0 3.6864*10**6
8 1924 10.60 20394.4 3.7018*10**6
9 1928 10.80 20822.4 3.7172*10**6
10 1932 10.30 19899.6 3.7326*10**6
11 1936 10.30 19940.8 3.7481*10**6
12 1948 10.30 20064.4 3.7947*10**6
13 1952 10.40 20300.8 3.8103*10**6
14 1956 10.50 20538.0 3.8259*10**6
15 1960 10.20 19992.0 3.8416*10**6
16 1964 10.00 19640.0 3.8573*10**6
17 1968 9.95 19581.6 3.8730*10**6
18 1972 10.14 19996.1 3.8888*10**6
19 1976 10.06 19878.6 3.9046*10**6
20 1980 10.25 20295.0 3.9204*10**6
21 1984 9.99 19820.2 3.9363*10**6
22 1988 9.92 19721.0 3.9521*10**6
23 1992 9.96 19840.3 3.9681*10**6
24 1996 9.84 19640.6 3.9840*10**6
25 2000 9.87 19740.0 4.0000*10**6
26 2004 9.85 19739.4 4.0160*10**6
27 2008 9.69 19457.5 4.0321*10**6
```

```python
# python3
import numpy as np
import pandas as pd
import copy as copy
import matplotlib.pyplot as plt

df = pd.read_csv('data.csv',sep=' ')
mm = copy.deepcopy(df)
for index, row in df.iterrows():
    df.loc[index,'x2'] =  eval(df.iloc[index]['x2'])
    print(int(df.iloc[index]['x2']))

mean = df.mean()
w1 = (mean['xntn']-mean['xn']*mean['tn'])/(mean['x2'] -mean['xn']*mean['xn'])
w0 = mean['tn'] - w1*mean['xn']
xn = mm.loc[:,'xn']
tn = mm.loc[:,'tn']
x = np.linspace(1870,2020,100) # 100 linearly spaced numbers
y = w0 + w1*x
plt.xlabel('Years', fontsize = 30)
plt.ylabel('Time(second)', fontsize = 30)
plt.plot(xn, tn, 'ro')
plt.plot(x,y)
plt.show()
```

### Result:
{% katexmm display %}
$f \left( x ; w _ { 0 } , w _ { 1 } \right) = 36.416 - 0.013 x$ 
![](/assets/img/ML/linear_model/linear_model.png)
*Linear model*

## Vector/ Matrix notation
why?
   * To express many variables in one time
example:
   * $f \left( x _ { n } ; w _ { 0 } , w _ { 1 } \right) = \mathbf { w } ^ { \top } \mathbf { x } _ { n } = w _ { 0 } + w _ { 1 } x _ { n }$
   * $\mathcal { L } = \frac { 1 } { N } ( \mathbf { t } - \mathbf { X w } ) ^ { \top } ( \mathbf { t } - \mathbf { X } \mathbf { w } )$
   * $\mathbf { X } = \left[ \begin{array} { c } { \mathbf { x } _ { 1 } ^ { \top } } \\ { \mathbf { x } _ { 2 } ^ { \top } } \\ { \vdots } \\ { \mathbf { x } _ { N } ^ { \top } } \end{array} \right] = \left[ \begin{array} { c c } { 1 } &amp; { x _ { 1 } } \\ { 1 } &amp; { x _ { 2 } } \\ { \vdots } &amp; { \vdots } \\ { 1 } &amp; { x _ { N } } \end{array} \right] , \quad \mathbf { t } = \left[ \begin{array} { c } { t _ { 1 } } \\ { t _ { 2 } } \\ { \vdots } \\ { t _ { N } } \end{array} \right]$
   * $\mathbf { w } = \left[ \begin{array} { c } { w _ { 0 } } \\ { w _ { 1 } } \end{array} \right] , \quad \mathbf { x } _ { n } = \left[ \begin{array} { c } { 1 } \\ { x _ { n } } \end{array} \right]$
   * $\frac { \partial \mathcal { L } } { \partial \mathbf { w } } = \left[ \begin{array} { l } { \frac { \partial \mathcal { L } } { \partial w _ { 0 } } } \\ { \frac { \partial \mathcal { L } } { \partial w _ { 1 } } } \end{array} \right]$

useful identities:
$\begin{array} { c | c } f ( \mathbf { w } ) &amp; \quad \frac { \partial f } { \partial \mathbf { w } }\\{ \mathbf { w } ^ { \top } \mathbf { x } } &amp; { \mathbf { x } }\\{ \mathbf { x } ^ { \top } \mathbf { w } } &amp; { \mathbf { x } } \\ { \mathbf { w } ^ { \top } \mathbf { w } } &amp; { 2 \mathbf { w } } \\ { \mathbf { w } ^ { \top } \mathbf { C } \mathbf { w } } &amp; { 2 \mathbf { C } \mathbf { w } } \end{array}$

### Squre root

$\widehat { \mathbf { w } } = \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t }$

```python
# data is the same as above
import numpy as np
import pandas as pd
import copy as copy
import matplotlib.pyplot as plt

df = pd.read_csv('data.csv',sep=' ')
mm = copy.deepcopy(df)
for index, row in df.iterrows():
    df.loc[index,'x2'] =  eval(df.iloc[index]['x2'])
    #print(int(df.iloc[index]['x2']))

mean = df.mean()
X = np.ones(mm.shape[0]*2).reshape(mm.shape[0],2)
t = np.ones(mm.shape[0]).reshape(mm.shape[0],1)
for index in range(X.shape[0]):
    X[index][1] = mm.loc[index, 'xn']
    t[index][0] = mm.loc[index, 'tn']
Xt = np.transpose(X)
w = np.dot(np.dot(np.linalg.inv(np.dot(Xt, X)) , Xt),t)
xn = mm.loc[:,'xn']
tn = mm.loc[:,'tn']
x = np.linspace(1870,2020,100) 
y = w[0] + w[1]*x
plt.xlabel('Years', fontsize = 30)
plt.ylabel('Time(second)', fontsize = 30)
plt.plot(xn, tn, 'ro')
plt.plot(x,y)
plt.show()

# print(w)
# out: array([[3.64164559e+01],[-1.33308857e-02]])
# the same result with above
```

```python
# another better method and poly
import pylab as plt
import urllib.request
# You only need this line if you haven't cloned the repo...if you have cloned, you'll already have the data
urllib.request.urlretrieve('https://raw.githubusercontent.com/sdrogers/fcmlcode/master/notebooks/data/olympic100m.txt', 'olympic100m.txt')
import numpy as np
# If you have cloned, make sure this is pointing to the correct file, maybe ../data/olympic100m.txt ?
data = np.loadtxt('olympic100m.txt',delimiter=',')
x = data[:,0][:,None]
t = data[:,1][:,None]


X = np.hstack((np.ones_like(x),x,x**2))  # we just add more to here to change all 
X = np.asmatrix(X)
t = t # This is already a vector!

w = (X.T*X).I*X.T*t
testx = np.linspace(1896,2012,100)[:,None]
testX = np.hstack((np.ones_like(testx),testx,testx**2))
testt = testX*w
plt.figure()
plt.plot(x,t,'ro')
plt.plot(testx,testt,'b')
plt.show()

```
...

![](/assets/img/ML/linear_model/poly2.png)
*Poly = 2*
![](/assets/img/ML/linear_model/poly3.png)
*Poly = 3*

we are free to define any set of K functions of X
$\mathbf { X } = \left[ \begin{array} { c c c c } { h _ { 1 } \left( x _ { 1 } \right) } &amp; { h _ { 2 } \left( x _ { 1 } \right) } &amp; { \cdots } &amp; { h _ { K } \left( x _ { 1 } \right) } \\ { h _ { 1 } \left( x _ { 2 } \right) } &amp; { h _ { 2 } \left( x _ { 2 } \right) } &amp; { \cdots } &amp; { h _ { K } \left( x _ { 2 } \right) } \\ { \vdots } &amp; { \vdots } &amp; { \cdots } &amp; { \vdots } \\ { h _ { 1 } \left( x _ { N } \right) } &amp; { h _ { 2 } \left( x _ { N } \right) } &amp; { \cdots } &amp; { h _ { K } \left( x _ { N } \right) } \end{array} \right]$
### Like:

```python
import pylab as plt
import urllib.request
# You only need this line if you haven't cloned the repo...if you have cloned, you'll already have the data
urllib.request.urlretrieve('https://raw.githubusercontent.com/sdrogers/fcmlcode/master/notebooks/data/olympic100m.txt', 'olympic100m.txt')
import numpy as np
import math
# If you have cloned, make sure this is pointing to the correct file, maybe ../data/olympic100m.txt ?
data = np.loadtxt('olympic100m.txt',delimiter=',')
x = data[:,0][:,None]
t = data[:,1][:,None]
hx = np.sin((x-2660)/4.3)

X = np.hstack((np.ones_like(x),x,hx))  # we just add more to here to change all 
X = np.asmatrix(X)
t = t # This is already a vector!

w = (X.T*X).I*X.T*t
testx = np.linspace(1896,2012,100)[:,None]
testX = np.hstack((np.ones_like(testx),testx,np.sin((testx-2660)/4.3)))
testt = testX*w
plt.figure()
plt.plot(x,t,'ro')
plt.plot(testx,testt,'b')
plt.show()
```
...
![](/assets/img/ML/linear_model/polynomial_model.png)
*Polynomial model*


## Validation data

1. Why we need to validate?
   * to restrain overfitting
   
2. How we validate data?
   * a effecient technique -- Cross-validation(K-fold)
   
3. How to prevent from overfitting?
   * regularisation- if we don't want our model to become too complex
     * $\mathcal { L } ^ { \prime } = \mathcal { L } + \lambda \mathbf { w } ^ { \top } \mathbf { w }$
     * $\widehat { \mathbf { w } } = \left( \mathbf { X } ^ { \top } \mathbf { X } + N \lambda \mathbf { I } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t }$

```python
import pylab as plt
import urllib.request
# You only need this line if you haven't cloned the repo...if you have cloned, you'll already have the data
urllib.request.urlretrieve('https://raw.githubusercontent.com/sdrogers/fcmlcode/master/notebooks/data/olympic100m.txt', 'olympic100m.txt')
import numpy as np
import math
# If you have cloned, make sure this is pointing to the correct file, maybe ../data/olympic100m.txt ?
data = np.loadtxt('olympic100m.txt',delimiter=',')
x = data[:,0][:,None]
t = data[:,1][:,None]
maxorder = 7
x_test = np.linspace(1896,2020,100)[:,None]
X = np.ones_like(x)
X_test = np.ones_like(x_test)
for i in range(1,maxorder+1):
    X = np.hstack((X,x**i))
    X_test = np.hstack((X_test,x_test**i))

X = np.asmatrix(X)
for lamb in [0,0.01,0.1,1,10,100]:
    w = np.linalg.solve(np.dot(X.T,X) + x.size*lamb*np.identity(maxorder+1),np.dot(X.T,t))
    A = np.dot(X.T,X) 
   
    B = np.dot(X.T,t)
    m = A.I*B
    #w = np.linalg.solve(A,B)
    f_test = np.dot(X_test,w)
    plt.figure()
    plt.plot(x_test,f_test,'b-',linewidth=2)
    plt.plot(x,t,'ro')
    title = '$\lambda=$%g'%lamb
    plt.title(title)

```
![](/assets/img/ML/linear_model/lamda0.01.png)
*$lambda = 0.01$*
![](/assets/img/ML/linear_model/lamda0.1.png)
*$lambda = 0.1$*
![](/assets/img/ML/linear_model/lamda1.png)
*$lambda = 1$*
![](/assets/img/ML/linear_model/lamda10.png)
*$lambda = 10$*



**Attention**: use np.linalg.solve(A,B) instead of $w = A ^ { - 1 }*B$ becauese the accuracy problem.

### Cross-validation
The loss that we calculate from validation data will be sensitive to the choice of data in our validation set, especially dataset is small. so we need a method to make more efficient use of the data we have
K-fold cross-validation, particularly, Leave-One-Out Cross-Validation(LOOCV)
$$\mathcal { L } ^ { C V } = \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \left( t _ { n } - \widehat { \mathbf { w } } _ { - n } ^ { \top } \mathbf { x } _ { n } \right) ^ { 2 }$$

### The best model -- minimize test loss

```python
import pylab as plt
import urllib.request
# You only need this line if you haven't cloned the repo...if you have cloned, you'll already have the data
urllib.request.urlretrieve('https://raw.githubusercontent.com/sdrogers/fcmlcode/master/notebooks/data/olympic100m.txt', 'olympic100m.txt')
import numpy as np
import math
# If you have cloned, make sure this is pointing to the correct file, maybe ../data/olympic100m.txt ?
data = np.loadtxt('olympic100m.txt',delimiter=',')
x = data[:,0][:,None]
t = data[:,1][:,None]
maxorder = 8
x_train = x[:-3]
x_test = x[-3:]
t_train = t[:-3]
t_test = t[-3:]
X_train = np.ones_like(x_train)
X_test = np.ones_like(x_test)
errorlist = []
errorstdlist = []
for i in range(1,maxorder+1):
    X_train = np.hstack((X_train,x_train**i))
    X_test = np.hstack((X_test,x_test**i))
    w = np.linalg.solve(np.dot(X_train.T,X_train) ,np.dot(X_train.T,t_train))
    #w = np.linalg.solve(A,B)
    f_test = np.dot(X_test,w)
    error_std = np.std(f_test - t_test)
    errorstdlist.append(np.std(np.dot(X_train,w) - t_train))
    errorlist.append (error_std)
    f_test = np.dot(X_test,w)
plt.figure()
plt.plot([1,2,3,4,5,6,7,8],errorlist,'b-',linewidth=2)
plt.plot([1,2,3,4,5,6,7,8],errorstdlist,'y-',linewidth=2)
plt.show()
```
![](/assets/img/ML/linear_model/best_model_visualization.png)
*Best model visualization*
Then we can choose the best model.

### LOOCV loss and train, test less
LOOCV loss:
![](/assets/img/ML/linear_model/LOOCV_loss.png)



{% endkatexmm %}</content><author><name>Linfeng Chen</name></author><category term="Tech" /><category term="Machine Learning" /><category term="Math" /><summary type="html">The essentials Choose w to get the min Loss function.</summary></entry><entry xml:lang="en"><title type="html">Linear Modeling - A Maximum Likelihood Approach</title><link href="http://find1dream.github.io/en/Linear_modeling_Maximum_likelihood_approach" rel="alternate" type="text/html" title="Linear Modeling - A Maximum Likelihood Approach" /><published>2019-09-06T00:00:00+09:00</published><updated>2019-09-06T00:00:00+09:00</updated><id>http://find1dream.github.io/en/Linear_modeling_Maximum_likelihood_approach</id><content type="html" xml:base="http://find1dream.github.io/en/Linear_modeling_Maximum_likelihood_approach">## ERRORS AS NOISE
Explicitly model the noise (the errors between the model and the observations) in the data by incorporating a random variable.

{% katexmm display %}
### Advantages of incorporating a noise term into our model:
* Allows us to express the level of uncertainty in our estimate of the model parameters, $w$
* Provides several benefits over loss minimisation.
* Able to quantify the uncertainty in our parameters and the ability to express uncertainties in our predictions(just use the available).

### How to present linear model that fit our data
For linear model, we can add a variable to think in a generative way.
$t _ { n } = \mathbf { w } ^ { T } \mathbf { x } _ { n } + \epsilon _ { n }$
$\epsilon _ { n }$ is continuous random variable

also, $\epsilon _ { n }$ should be independent, which means:
$p \left( \epsilon _ { 1 } , \ldots , \epsilon _ { N } \right) = \prod _ { n = 1 } ^ { N } p \left( \epsilon _ { n } \right)$. Usually, we use gaussian distribution: $p ( \epsilon ) = \mathcal { N } \left( \mu , \sigma ^ { 2 } \right)$

```python
mu, sigma = 0, 0.1 # mean and standard deviation
s = np.random.normal(mu, sigma, 1000)
print(abs(mu - np.mean(s)) &lt; 0.01)
print(abs(sigma - np.std(s, ddof=1)) &lt; 0.01)
import matplotlib.pyplot as plt
count, bins, ignored = plt.hist(s, 30, density=True)
plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ),linewidth=2, color='r')
plt.show()
##out 
#true
#true
```

![](/assets/img/ML/linear_model/guassian_dis.png)
*Guassian Distribution*

```python
import numpy as np
mu, sigma = 0, 0.05 # mean and standard deviation
s = np.random.normal(mu, sigma, 10)
x = np.vstack(np.linspace(0,9,num=10))
w = np.array([-0.05,9.375])
xx = np.hstack((x,np.ones_like(x)))
y = xx @ w + s
plt.scatter(x.T,y)
plt.show()
```

![](/assets/img/ML/linear_model/linear_sample.png)
*Linear sample*

### Dataset likelihood
Usually we are not interested in the likelihood of a single data point but that of all of the data, like:

$p \left( t _ { 1 } , \ldots , t _ { N } | \mathbf { x } _ { 1 } , \ldots , \mathbf { x } _ { N } , \mathbf { w } , \sigma ^ { 2 } \right)$ for short $p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } )$

We assume $p$ is gaussian distribution, and noise at each point is completely independent, then\\
$L = p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) = \prod _ { n = 1 } ^ { N } p \left( t _ { n } | \mathbf { x } _ { n } , \mathbf { w } , \sigma ^ { 2 } \right) = \prod _ { n = 1 } ^ { N } \mathcal { N } \left( \mathbf { w } ^ { \top } \mathbf { x } _ { n } , \sigma ^ { 2 } \right)$  (*)

$t _{n}$ are conditionally independent -- given a value for $w$, the $t _{n}$ are independent
Eg: To get $p \left( t _ { 1 } 960 | \mathbf { x } _ { 1960 } , \mathbf { X } , \mathbf { t } \right)$

we can have $p \left( t _ { 1960 } | \mathbf { x } _ { 1960 } , \mathbf { X } , \mathbf { t } \right) = \frac { p \left( t _ { 1960 } , \mathbf { t } | \mathbf { x } _ { 1960 } , \mathbf { X } \right) } { p ( \mathbf { t } | \mathbf { X } ) }$

then\\ $p \left( t _ { 1960 } | \mathbf { x } _ { 1960 } , \mathbf { X } , \mathbf { t } \right) = \frac { p \left( t _ { 1960 } | \mathbf { x } _ { 1060 } \right) \prod _ { n } p \left( t _ { n } | \mathbf { x } _ { n } \right) } { \prod _ { n } p \left( t _ { n } | \mathbf { x } _ { n } \right) } = p \left( t _ { 1960 } | \mathbf { x } _ { 1960 } \right)$

that shows in our model, $t _{n}$ are conditionally independent -- given a value for $w$, the $t _{n}$ are independent. This dependence is encapsulated in the parameter $w$.
In a word, if we know $w$, all that remains is the errors between the observed data and $w ^{T}x_{n}$, errors are independent(noise--gaussian distribution),conditioned on w, the observations are independent. Without a model (and therefore a w), the observations are not independent.
### Maximum likelihood
#### Why we need to maximum the likelihood?
Because we have a fixed dataset, varying the model will result in different likelihood values, so a sensible choice of model would be that which maximised the likelihood ==&gt; choose the $w$ and $\sigma ^{2}$ to maximum $L$.

$$\begin{aligned} \log L &amp; = \sum _ { n = 1 } ^ { N } \log \left( \frac { 1 } { \sqrt { 2 \pi \sigma ^ { 2 } } } \exp \left\{ - \frac { 1 } { 2 \sigma ^ { 2 } } \left( t _ { n } - f \left( \mathbf { x } _ { n } ; \mathbf { w } \right) \right) ^ { 2 } \right\} \right) \\ &amp; = \sum _ { n = 1 } ^ { N } \left( - \frac { 1 } { 2 } \log ( 2 \pi ) - \log \sigma - \frac { 1 } { 2 \sigma ^ { 2 } } \left( t _ { n } - f \left( \mathbf { x } _ { n } , \mathbf { w } \right) \right) ^ { 2 } \right) \\ &amp; = - \frac { N } { 2 } \log 2 \pi - N \log \sigma - \frac { 1 } { 2 \sigma ^ { 2 } } \sum _ { n = 1 } ^ { N } \left( t _ { n } - f \left( \mathbf { x } _ { n } ; \mathbf { w } \right) \right) ^ { 2 } \end{aligned}$$

We know that $logL$ is a multivariable function that looks like:$f(w,\sigma)$, so we can deriviate $logL$ to find $w,\sigma$ corresponding to the maximum $logL$


$\begin{aligned} \frac { \partial \log L } { \partial \mathbf { w } } &amp; = \frac { 1 } { \sigma ^ { 2 } } \sum _ { n = 1 } ^ { N } \mathbf { x } _ { n } \left( t _ { n } - \mathbf { x } _ { n } ^ { \top } \mathbf { w } \right) \\ &amp; = \frac { 1 } { \sigma ^ { 2 } } \sum _ { n = 1 } ^ { N } \mathbf { x } _ { n } t _ { n } - \mathbf { x } _ { n } \mathbf { x } _ { n } ^ { \top } \mathbf { w } = \mathbf { 0 } \end{aligned}$

$\mathbf { X } = \left[ \begin{array} { c } { \mathbf { x } _ { 1 } ^ { \top } } \\ { \mathbf { x } _ { 2 } ^ { \top } } \\ { \vdots } \\ { \mathbf { x } _ { N } ^ { \top } } \end{array} \right] = \left[ \begin{array} { c c } { 1 } &amp; { x _ { 1 } } \\ { 1 } &amp; { x _ { 2 } } \\ { \vdots } &amp; { \vdots } \\ { 1 } &amp; { x _ { N } } \end{array} \right] , \quad \mathbf { t } = \left[ \begin{array} { c } { t _ { 1 } } \\ { t _ { 2 } } \\ { \vdots } \\ { t _ { N } } \end{array} \right]$

$\sum _ { n = 1 } ^ { N } \mathbf { x } _ { n } t _ { n }$ can be write as $X ^ {T}t $
and $\sum _ { n = 1 } ^ { N } \mathbf { x } _ { n } \mathbf { x } _ { n } ^ { \top } \mathbf { w }$ can be write as $X^{T}Xw$
then\\

$\begin{aligned}\frac { \partial \log L } { \partial \mathbf { w } } = \frac { 1 } { \sigma ^ { 2 } } \left( \mathbf { X } ^ { \top } \mathbf { t } - \mathbf { X } ^ { \top } \mathbf { X } \mathbf { w } \right) = \mathbf { 0 }\end{aligned}$
$\Rightarrow$ $\mathbf { w } = \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t }$
the same as the result we got in the previous chapter.

Also for $\sigma$ \\
$\frac { \partial \log L } { \partial \sigma } = - \frac { N } { \sigma } + \frac { 1 } { \sigma ^ { 3 } } \sum _ { n = 1 } ^ { N } \left( t _ { n } - \mathbf { x } ^ { \top } \widehat { \mathbf { w } } \right) ^ { 2 } = 0$\\
$$\begin{aligned} \sigma ^ { 2 } &amp; = \frac { 1 } { N } ( \mathbf { t } - \mathbf { X } \widehat { \mathbf { w } } ) ^ { \top } ( \mathbf { t } - \mathbf { X } \widehat { \mathbf { w } } ) \\ &amp; = \frac { 1 } { N } \left( \mathbf { t } ^ { \top } \mathbf { t } - 2 \mathbf { t } ^ { \top } \mathbf { X } \widehat { \mathbf { w } } + \widehat { \mathbf { w } } ^ { \top } \mathbf { X } ^ { \top } \mathbf { X } \widehat { \mathbf { w } } \right) \end{aligned}$$\\
$$\begin{aligned} \widehat { \sigma ^ { 2 } } &amp; = \frac { 1 } { N } \left( \mathbf { t } ^ { \top } \mathbf { t } - 2 \mathbf { t } ^ { \top } \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t } + \mathbf { t } ^ { \top } \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t } \right) \\ &amp; = \frac { 1 } { N } \left( \mathbf { t } ^ { \top } \mathbf { t } - 2 \mathbf { t } ^ { \top } \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t } + \mathbf { t } ^ { \top } \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t } \right) \\ &amp; = \frac { 1 } { N } \left( \mathbf { t } ^ { \top } \mathbf { t } - \mathbf { t } ^ { \top } \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t } \right) \end{aligned}$$

```python
import pylab as plt
import urllib.request
urllib.request.urlretrieve('https://raw.githubusercontent.com/sdrogers/fcmlcode/master/notebooks/data/olympic100m.txt', 'olympic100m.txt')
import numpy as np
import math
data = np.loadtxt('olympic100m.txt',delimiter=',')
x = data[:,0][:,None]
t = data[:,1][:,None]
X = np.hstack((np.ones_like(x),x))  # we just add more to here to change all 
X = np.asmatrix(X)
t = t # This is already a vector!
w = (X.T*X).I*X.T*t
delta = (1.0/(len(x)-1)) * (t.T @ t - t.T @ X @ w)  # unbaised
#output
w = [36.4, -1.33*e-2]
delta = 0.052
```
$ \widehat { \sigma ^ { 2 } } = \frac { 1 } { N } \left( \mathbf { t } ^ { \top } \mathbf { t } - \mathbf { t } ^ { \top } \mathbf { X } \widehat { \mathbf { w } } \right)$

But!!!
To ensure we have found the maximum, the hessian matrix should be negative definite
$\mathbf { H } = \left[ \begin{array} { c c c c } { \frac { \partial ^ { 2 } f } { \partial w _ { 1 } ^ { 2 } } } &amp; { \frac { \partial ^ { 2 } f } { \partial w _ { 1 } \partial w _ { 2 } } } &amp; { \cdots } &amp; { \frac { \partial ^ { 2 } f } { \partial w _ { 1 } \partial w _ { K } } } \\ { \frac { \partial ^ { 2 } f } { \partial w _ { 2 } \partial w _ { 1 } } } &amp; { \frac { \partial ^ { 2 } f } { \partial w _ { 2 } ^ { 2 } } } &amp; { \cdots } &amp; { \frac { \partial ^ { 2 } f } { \partial w _ { 2 } \partial w _ { K } } } \\ { \vdots } &amp; { \vdots } &amp; { \ddots } &amp; { \vdots } \\ { \frac { \partial ^ { 2 } f } { \partial w _ { K } \partial w _ { 1 } } } &amp; { \frac { \partial ^ { 2 } f } { \partial w _ { K } \partial w _ { 2 } } } &amp; { \cdots } &amp; { \frac { \partial ^ { 2 } f } { \partial w _ { K } ^ { 2 } } } \end{array} \right]$

The Hessian matrix of second-order partial derivatives can be computed by differentiating $\frac { \partial \log L } { \partial w }$ with respect to $w^{T}$:

$\frac { \partial ^ { 2 } \log L } { \partial \mathbf { w } \partial \mathbf { w } ^ { \top } } = - \frac { 1 } { \sigma ^ { 2 } } \mathbf { X } ^ { \top } \mathbf { X }$

```python
#calculate Hessian matrix
import pylab as plt
import urllib.request
urllib.request.urlretrieve('https://raw.githubusercontent.com/sdrogers/fcmlcode/master/notebooks/data/olympic100m.txt', 'olympic100m.txt')
import numpy as np
import math
data = np.loadtxt('olympic100m.txt',delimiter=',')
x = data[:,0][:,None]
t = data[:,1][:,None]
hx = np.sin((x-2660)/4.3)

X = np.hstack((np.ones_like(x),x))  # we just add more to here to change all 
X = np.asmatrix(X)
t = t # This is already a vector!

w = (X.T*X).I*X.T*t
delta = (1.0/(len(x)-1)) * (t.T @ t - t.T @ X @ w)

H = -1.0/(delta[0,0]**2) * X.T @ X
```

for any vector $z$, we can see that $\mathbf { z } ^ { \top } \mathbf { X } ^ { \top } \mathbf { X } \mathbf { z } &gt; 0$. because $Xz$ is a vector

Also, for $\widehat { \sigma ^ { 2 } }$,

$\frac { \partial ^ { 2 } \log L } { \partial \sigma ^ { 2 } } = \frac { N } { \sigma ^ { 2 } } - \frac { 3 } { \sigma ^ { 4 } } ( \mathbf { t } - \mathbf { X } \widehat { \mathbf { w } } ) ^ { \top } ( \mathbf { t } - \mathbf { X } \widehat { \mathbf { w } } )$


$\begin{aligned} \frac { \partial ^ { 2 } \log L } { \partial \sigma ^ { 2 } } &amp; = \frac { N } { \widehat { \sigma } ^ { 2 } } - \frac { 3 } { \left( \widehat { \sigma ^ { 2 } } \right) ^ { 2 } } N \widehat { \sigma ^ { 2 } } \\ &amp; = - \frac { 2 N } { \widehat { \sigma } ^ { 2 } } \end{aligned} &lt; 0 $
$\Rightarrow$ $\widehat { \sigma ^ { 2 } }$ corresponds to a maximum.


### Maximum likelihood favours complex models
#### Why?
$\begin{aligned} \log L &amp; = - \frac { N } { 2 } \log 2 \pi - \frac { N } { 2 } \log \widehat { \sigma ^ { 2 } } - \frac { 1 } { 2 \widehat { \sigma ^ { 2 } } } N \widehat { \sigma ^ { 2 } } \\ &amp; = - \frac { N } { 2 } ( 1 + \log 2 \pi ) - \frac { N } { 2 } \log \widehat { \sigma ^ { 2 } } \end{aligned}$
that means if we decrease $\widehat { \sigma ^ { 2 } }$, we can get a bigger $L$, which means we have a more complex model. The more complex the model is, the more likely it is, if we were to use log L to help choose which particular model to use, it would always point us to models of increasing complexity !!! But in real world, it's obviously not this case. In this situation, we need to trade off beween generalisation and over-fitting(bias-variance trade-off).
#### How to solve this problem?
* regularisation
* use prior distribution on the parameter values
* $\overline { \mathcal { M } } = \mathcal { B } ^ { 2 } + \mathcal { V }$, bias $\mathcal {B}$, and variance $\mathcal {V}$
  * a model that is too simple will have a high bias(underfitting)
  * a model that is too complex will have a high variance
  
### Effect of noise on parameter estimates
#### Uncertainty is estimated
$\widehat {w }$ is strongly influenced by the particular noise in the data $\Rightarrow$ it's useful to know how much uncertainty there was in $\widehat { w  }$, in general, we can calculate the expectation($\mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \{ \widehat { \mathbf { w } } \}$) to show it's uncertainty--given the result distribution, how it's likely that  $\hat {w}$ 

We already know: $p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) = \prod _ { n = 1 } ^ { N } p \left( t _ { n } | \mathbf { x } _ { n } \mathbf { w } , \sigma ^ { 2 } \right) = \prod _ { n = 1 } ^ { N } \mathcal { N } \left( \mathbf { w } ^ { \top } \mathbf { x } _ { n } , \sigma ^ { 2 } \right)$

then\\  $p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) = \mathcal { N } \left( \mathbf { X } \mathbf { w } , \sigma ^ { 2 } \mathbf { I } \right)$

$\Rightarrow$ $\mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \{ \widehat { \mathbf { w } } \} = \int \widehat { \mathbf { w } } p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) d \mathbf { t }$
and $\widehat { \mathbf { w } } = \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t }$
$\Rightarrow$ $\mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \{ \widehat { \mathbf { w } } \}$ $\begin{aligned} &amp; = \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \int \mathbf { t } p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) d t \\ &amp; = \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \{ \mathbf { t } \} \\ &amp; = \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { X } \mathbf { w } \\ &amp; = \mathbf { w } \end{aligned}$
That means our estimation to $\hat {w}$ is unbiased.

What covariance matrix can represent:
* diagonal elements: parameters variability
* how parameters co-vary
Eg:
![](/assets/img/ML/linear_model/likelihood_function.png)
*Likelihood function*
In the picture above, increasing $\mathit{w1}$ causes a decrease in $w _{0}$ so we might expect the off-diagonal elements in the covariance matrix to be negative.

**Cov**
$p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) = \mathcal { N } \left( \mathbf { X } \mathbf { w } , \sigma ^ { 2 } \mathbf { I } \right)$ $\Rightarrow$ covariance of $t$ is $\sigma ^ { 2 } \mathbf { I }$ and its mean is $Xw$.

$\begin{aligned} \operatorname { cov } \{ \widehat { \mathbf { w } } \} &amp; = \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ \widehat { \mathbf { w } } \widehat { \mathbf { w } } ^ { \top } \right\} - \mathbf { E } _ { p ( \mathbf { t } | \mathbf { x } , \mathbf { w } , \sigma ^ { 2 } ) } \{ \widehat { \mathbf { w } }  \}  \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \{ \widehat { \mathbf { w } } \} ^ { \top } \\ &amp; = \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ \widehat { \mathbf { w } } \widehat { \mathbf { w } } ^ { \top } \right\} - \mathbf { w } \mathbf { w } ^ { \top } \end{aligned}$

$\mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ \widehat { \mathbf { w } } \widehat { \mathbf { w } } ^ { \top } \right\} = \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left \{ \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t } \right) \left( \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t } \right) ^ { \top } \} $
$= \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ \mathbf { t } \mathbf { t } ^ { \top } \right\} \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 }$
$\operatorname { cov } \{ \mathbf { t } \} = \sigma ^ { 2 } \mathbf { I } = \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ \mathbf { t } \mathbf { t } ^ { \top } \right\} - \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \{ \mathbf { t } \} \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \{ \mathbf { t } \} ^ { \top }$

then:\\
$\begin{aligned} \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ \widehat { \mathbf { w } } \widehat { \mathbf { w } } ^ { \top } \right\} = &amp; \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { X } \mathbf { w } ^ { \top } \mathbf { X } ^ { \top } \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \\ &amp; + \sigma ^ { 2 } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \end{aligned}$
$= \quad \mathbf { w } \mathbf{ w } ^ { T } + \sigma ^ { 2 } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 }$
$\begin{aligned} \operatorname { cov } \{ \widehat { \mathbf { w } } \} &amp; = \mathbf { w } \mathbf { w } ^ { \top } + \sigma ^ { 2 } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } - \mathbf { w } \mathbf { w } ^ { \top } \\ &amp; = \sigma ^ { 2 } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \end{aligned}$


```python
import pylab as plt
import urllib.request
urllib.request.urlretrieve('https://raw.githubusercontent.com/sdrogers/fcmlcode/master/notebooks/data/olympic100m.txt', 'olympic100m.txt')
import numpy as np
import math
data = np.loadtxt('olympic100m.txt',delimiter=',')
x = data[:,0][:,None]
t = data[:,1][:,None]
hx = np.sin((x-2660)/4.3)

X = np.hstack((np.ones_like(x),x))  # we just add more to here to change all 
X = np.asmatrix(X)
t = t # This is already a vector!
w = (X.T*X).I*X.T*t
delta2 = (1.0/(len(x)-1)) * (t.T @ t - t.T @ X @ w)
cov = delta2 * (X.T@X).I
```

Hessian matrix of second-order partial derivatives:\\
$\frac { \partial ^ { 2 } \log L } { \partial \mathbf { w } \partial \mathbf { w } ^ { T } } = - \frac { 1 } { \sigma ^ { 2 } } \mathbf { X } ^ { \top } \mathbf { X }$
which means: low curvature corresponds to a high level of uncertainty in parameters and high curvature to a low level.

Fisher Information Matrix:\\
$\mathcal { I } = \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ - \frac { \partial ^ { 2 } \log p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } { \partial \mathbf { w } \partial \mathbf { w } ^ { \top } } \right\}$
then\\
$\mathcal { I } = \frac { 1 } { \sigma ^ { 2 } } \mathbf { X } ^ { \top } \mathbf { X }$
Information matrix means: if the data is noisy, we have lower information, but the variance is high, becuase:
$\operatorname { cov } \{ \widehat { \mathbf { w } } \} = \mathcal { I } ^ { - 1 } $

How to understand cov?
like the result of the code above, we can get $\operatorname { cov } \{ \widehat { \mathbf { w } } \} = \left[ \begin{array} { c c } { 5.7972 } &amp; { - 0.0030 } \\ { - 0.0030 } &amp; { 1.5204 e - 06 } \end{array} \right]$
that means:
* diagonal elements: variance of $w _{0}$ is much higher than $w _{1}$, suggesting that we could tolerate bigger changes in $w _{0}$ than $w _{1}$ and still be left with a reasonably good model.
![](/assets/img/ML/linear_model/diagonal_elements.png)
*Diagonal Elements*
* off-diagonal elements: if we were to slightly increase either $w _{0}$ or $w _{1}$, we would have to slightly decrease the other
![](/assets/img/ML/linear_model/off_dig_elements.png)
*Off Diagonal Elements*

### Vairability in predictions
If we are quite certain about our prediction, this range might be small; if we are less certain, it might be large.
$t _ { \mathrm { new } } = \widehat { \mathbf { w } } ^ { \mathrm { T } } \mathbf { x } _ { \mathrm { new } }$
$\begin{aligned} \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ t _ { \mathrm { new } } \right\} &amp; = \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \{ \widehat { \mathbf { w } } \} ^ { \top } \mathbf { x } _ { \mathrm { new } } \\ &amp; = \mathbf { w } ^ { \top } \mathbf { x } _ { \mathrm { new } } \end{aligned}$

$\sigma _ { \mathrm { new } } ^ { 2 } = \operatorname { var } \left\{ t _ { \mathrm { new } } \right\} = \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ t _ { \mathrm { new } } ^ { 2 } \right\} - \left( \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ t _ { \mathrm { new } } \right\} \right) ^ { 2 }$

$\begin{aligned} \operatorname { var } \left\{ t _ { \text { new } } \right\} &amp; = \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ \left( \widehat { \mathbf { w } } ^ { \top } \mathbf { x } _ { \text { new } } \right) ^ { 2 } \right\} - \left( \mathbf { w } ^ { \top } \mathbf { x } _ { \text { new } } \right) ^ { 2 } \\ &amp; = \mathbf { E } _ { p ( \mathbf { t } | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ \mathbf { x } _ { \text { new } } ^ { \top } \widehat { \mathbf { w } } \widehat { \mathbf { w } } ^ { \top } \mathbf { x } _ { \text { new } } \right\} - \mathbf { x } _ { \text { new } } ^ { \top } \mathbf { w } \mathbf { w } ^ { \top } \mathbf { x } _ { \text { new. } } \end{aligned}$

$\operatorname { var } \left\{ t _ { \mathrm { new } } \right\} = \mathbf { x } _ { \mathrm { new } } ^ { \top } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { E } _ { p ( t | \mathbf { X } , \mathbf { w } , \sigma ^ { 2 } ) } \left\{ \mathbf { t } \mathbf { t } ^ { \top } \right\} \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { x } _ { \mathrm { new } } - \mathbf { x } _ { \mathrm { new } } ^ { \top } \mathbf { w } \mathbf { w } ^ { \top } \mathbf { x } _ { \mathrm { new } }$

$\operatorname { var } \left\{ t _ { \mathrm { new } } \right\} = \mathbf { x } _ { \mathrm { new } } ^ { \top } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \left( \sigma ^ { 2 } \mathbf { I } + \mathbf { X } \mathbf { w } \mathbf { w } ^ { \top } \mathbf { X } ^ { \top } \right) \mathbf { X } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { x } _ { \mathrm { new } } - \mathbf { x } _ { \mathrm { new } } ^ { \top } \mathbf { w } \mathbf { w } ^ { \top } \mathbf { x } _ { \mathrm { new } }$

$= \sigma ^ { 2 } \mathbf { x } _ { new } ^ { \top } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { x } _ { new } + \mathbf { x } _ { new } ^ { \top } \mathbf { w } \mathbf { w } ^ { \top } \mathbf { x } _ { new } - \mathbf { x } _ { new } ^ { \top } \mathbf { w } \mathbf { w } ^ { \top } \mathbf { x } _ { new } $
$= \sigma ^ { 2 } \mathbf { x } _ { new } ^ { \top } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { x } _ { new }$
$\Rightarrow$
**$\sigma _ { \mathrm { new } } ^ { 2 } = \mathbf { x } _ { \mathrm { new } } ^ { \mathrm { T } } \operatorname { cov } \{ \widehat { \mathbf { w } } \} \mathbf { x } _ { \mathrm { new } }$**
**$t _ { new } = \mathbf { x } _ { new } ^ { \top } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { X } ^ { \top } \mathbf { t } = \mathbf { x } _ { new } ^ { \top } \widehat { \mathbf { w } }$**
**$\sigma _ { new } ^ { 2 } = \sigma ^ { 2 } \mathbf { x } _ { new } ^ { \top } \left( \mathbf { X } ^ { \top } \mathbf { X } \right) ^ { - 1 } \mathbf { x } _ { new. }$**

A example: $f ( x ) = 5 x ^ { 3 } - x ^ { 2 } + x$, data points sampled from this function and corrupted by Gaussian noise with mean zero and variance 1000.test $t _ { \mathrm { new } } \pm \sigma _ { \mathrm { new } } ^ { 2 }$ for linear, cubic and sixthorder model.
* The increased variability in possible functions caused by the increase in parameter uncertainty is clear for the sixth-order model.
* For all models, the predictive variance increases as we move towards the edge of the data. The model is less confident in areas where it has less data.



![](/assets/img/ML/linear_model/all_models.png)
*All models*


code:
```python
import numpy as np
import random
import pylab as plt

def thirdfunc(x):
    return 5*x**3 -x**2 + x 

if __name__ == '__main__':
    x = np.random.uniform(low=-5,high=5,size=(50,))
    x.sort()
    mu, sigma = 0, 32
    s = np.random.normal(mu, sigma, len(x))
    y = thirdfunc(x) + s
    linex = np.linspace(-5,5,100)
    liney = thirdfunc(linex)
    plt.subplot(4,2,1)
    plt.scatter(x,y,color='blue')
    plt.plot(linex,liney, color='red')
    X = np.ones_like(x)
    inputx = x.reshape(len(x),1)
    X = X.reshape(len(X),1)
    Y = y.reshape(len(y),1)
    for i in range(1,7):
        X = np.hstack((X,inputx**i))
        w = np.linalg.solve(np.dot(X.T,X) ,np.dot(X.T,Y))
        print(&quot;estimate sigma'2: &quot;, (Y.T @ Y-Y.T@X@w)/(len(X)))
        sigma = (Y.T @ Y-Y.T@X@w)/(len(X))
        #plt.figure()
        plt.subplot(4, 2, i+1)
        #plt.plot(x,X@w)
        title = str(i) + &quot; rd&quot;
        plt.title(title)
        plt.scatter(x,y,color='blue')
        plt.plot(linex,liney, color='red')
        xnew = np.linspace(-5,5,100)
        xnew = xnew.reshape(len(xnew),1)
        xerror = np.ones_like(xnew)
        for m in range(1,i+1):
            xerror = np.hstack((xerror,xnew**m))
        inverse = np.linalg.inv(X.T@X)
        newsigma = sigma*xerror@inverse@xerror.T
        newsigma = np.diag(newsigma)
        plt.errorbar(xnew,xerror@w,yerr=newsigma)
    #w = np.linalg.solve(A,B)
    plt.show()

```

![](/assets/img/ML/linear_model/simulation_results.png)
*Simulation results*


{% endkatexmm %}</content><author><name>Linfeng Chen</name></author><category term="Tech" /><category term="Machine Learning" /><category term="Math" /><summary type="html">ERRORS AS NOISE Explicitly model the noise (the errors between the model and the observations) in the data by incorporating a random variable.</summary></entry><entry xml:lang="en"><title type="html">The Bayesian Approach To Machine Learning</title><link href="http://find1dream.github.io/en/Bayesian_approach_to_machine_learning" rel="alternate" type="text/html" title="The Bayesian Approach To Machine Learning" /><published>2019-09-04T00:00:00+09:00</published><updated>2019-09-04T00:00:00+09:00</updated><id>http://find1dream.github.io/en/Bayesian_approach_to_machine_learning</id><content type="html" xml:base="http://find1dream.github.io/en/Bayesian_approach_to_machine_learning">#### The likelihood
{% katexmm display %}
$p ( \mathbf { t } | \mathbf { w } , \mathbf { X } , \sigma ^ { 2 } ) = \mathcal { N } \left( \mathbf { X } \mathbf { w } , \sigma ^ { 2 } \mathbf { I } _ { N } \right)$
{% endkatexmm %}

#### The prior
{% katexmm display %}
$p ( \mathbf { w } | \boldsymbol { \mu } _ { 0 } , \mathbf { \Sigma } _ { 0 } ) = \mathcal { N } \left( \boldsymbol { \mu } _ { 0 } , \mathbf { \Sigma } _ { 0 } \right)$
We need to choose $\boldsymbol { \mu } _ { 0 }$ and $\boldsymbol { \Sigma } _ { 0 }$

#### The posterior
$\begin{aligned} p ( \mathbf { w } | \mathbf { t } , \mathbf { X } , \sigma ^ { 2 } ) &amp; \propto p ( \mathbf { t } | \mathbf { w } , \mathbf { X } , \sigma ^ { 2 } ) p ( \mathbf { w } | \boldsymbol { \mu } _ { 0 } ) \\ = &amp; \frac { 1 } { ( 2 \pi ) ^ { N / 2 } \left| \sigma ^ { 2 } \mathbf { I } \right| ^ { 1 / 2 } } \exp \left( - \frac { 1 } { 2 } ( \mathbf { t } - \mathbf { X } \mathbf { w } ) ^ { \top } \left( \sigma ^ { 2 } \mathbf { I } \right) ^ { - 1 } ( \mathbf { t } - \mathbf { X } \mathbf { w } ) \right) \\ &amp; \times \frac { 1 } { ( 2 \pi ) ^ { N / 2 } \left| \mathbf { \Sigma } _ { 0 } \right| ^ { 1 / 2 } } \exp \left( - \frac { 1 } { 2 } \left( \mathbf { w } - \boldsymbol { \mu } _ { 0 } \right) ^ { \top } \mathbf { \Sigma } _ { 0 } ^ { - 1 } \left( \mathbf { w } - \boldsymbol { \mu } _ { 0 } \right) \right) \end{aligned}$

$\begin{aligned} \propto &amp; \exp \left( - \frac { 1 } { 2 \sigma ^ { 2 } } ( \mathbf { t } - \mathbf { X } \mathbf { w } ) ^ { \top } ( \mathbf { t } - \mathbf { X } \mathbf { w } ) \right) \\ &amp; \times \exp \left( - \frac { 1 } { 2 } \left( \mathbf { w } - \boldsymbol { \mu } _ { 0 } \right) ^ { \top } \mathbf { \Sigma } _ { 0 } ^ { - 1 } \left( \mathbf { w } - \boldsymbol { \mu } _ { 0 } \right) \right) \\ = &amp; \exp \left\{ - \frac { 1 } { 2 } \left( \frac { 1 } { \sigma ^ { 2 } } ( \mathbf { t } - \mathbf { X } \mathbf { w } ) ^ { \top } ( \mathbf { t } - \mathbf { X } \mathbf { w } ) + \left( \mathbf { w } - \boldsymbol { \mu } _ { 0 } \right) ^ { \top } \mathbf { \Sigma } _ { 0 } ^ { - 1 } \left( \mathbf { w } - \boldsymbol { \mu } _ { 0 } \right) \right) \right\} \end{aligned}$

$p ( \mathbf { w } | \mathbf { t } , \mathbf { X } , \sigma ^ { 2 } ) \propto \exp \left\{ - \frac { 1 } { 2 } \left( - \frac { 2 } { \sigma ^ { 2 } } \mathbf { t } ^ { \top } \mathbf { X } \mathbf { w } + \frac { 1 } { \sigma ^ { 2 } } \mathbf { w } ^ { \top } \mathbf { X } ^ { \top } \mathbf { X } \mathbf { w } + \mathbf { w } ^ { \top } \mathbf { \Sigma } _ { 0 } ^ { - 1 } \mathbf { w } - 2 \boldsymbol { \mu } _ { 0 } ^ { \top } \mathbf { \Sigma } _ { 0 } ^ { - 1 } \mathbf { w } \right) \right\}$

$\begin{aligned} p ( \mathbf { w } | \mathbf { t } , \mathbf { X } , \sigma ^ { 2 } ) &amp; = \mathcal { N } \left( \boldsymbol { \mu } _ { \mathbf { w } } , \mathbf { \Sigma } _ { \mathbf { w } } \right) \\ &amp; \propto \exp \left( - \frac { 1 } { 2 } \left( \mathbf { w } - \boldsymbol { \mu } _ { \mathbf { w } } \right) ^ { \top } \mathbf { \Sigma } _ { \mathbf { w } } ^ { - 1 } \left( \mathbf { w } - \boldsymbol { \mu } _ { \mathbf { w } } \right) \right) \\ &amp; \propto \exp \left\{ - \frac { 1 } { 2 } \left( \mathbf { w } ^ { \top } \mathbf { \Sigma } _ { \mathbf { w } } ^ { - 1 } \mathbf { w } - 2 \boldsymbol { \mu } _ { \mathbf { w } } ^ { \top } \mathbf { \Sigma } _ { \mathbf { w } } ^ { - 1 } \mathbf { w } \right) \right\} \end{aligned}$

The terms linear and quadratic in w in equations must be equal, so\\
$\Rightarrow$
$\begin{aligned} \mathbf { w } ^ { \top } \mathbf { \Sigma } _ { \mathbf { w } } ^ { - 1 } \mathbf { w } &amp; = \frac { 1 } { \sigma ^ { 2 } } \mathbf { w } ^ { \top } \mathbf { X } ^ { \top } \mathbf { X } \mathbf { w } + \mathbf { w } ^ { \top } \mathbf { \Sigma } _ { 0 } ^ { - 1 } \mathbf { w } \\ &amp; = \mathbf { w } ^ { \top } \left( \frac { 1 } { \sigma ^ { 2 } } \mathbf { X } ^ { \top } \mathbf { X } + \mathbf { \Sigma } _ { 0 } ^ { - 1 } \right) \mathbf { w } \end{aligned}$
$\boldsymbol { \Sigma } _ { \mathbf { w } } = \left( \frac { 1 } { \sigma ^ { 2 } } \mathbf { X } ^ { \top } \mathbf { X } + \mathbf { \Sigma } _ { 0 } ^ { - 1 } \right) ^ { - 1 }$

and
$\begin{aligned} - 2 \boldsymbol { \mu } _ { \mathrm { w } } ^ { \top } \mathbf { \Sigma } _ { \mathrm { w } } ^ { - 1 } \mathbf { w } &amp; = - \frac { 2 } { \sigma ^ { 2 } } \mathbf { t } ^ { \top } \mathbf { X } \mathbf { w } - 2 \boldsymbol { \mu } _ { 0 } ^ { \top } \boldsymbol { \Sigma } _ { 0 } ^ { - 1 } \mathbf { w } \\ \boldsymbol { \mu } _ { \mathbf { w } } ^ { \top } \mathbf { \Sigma } _ { \mathbf { w } } ^ { - 1 } \mathbf { w } &amp; = \frac { 1 } { \sigma ^ { 2 } } \mathbf { t } ^ { \top } \mathbf { X } \mathbf { w } + \boldsymbol { \mu } _ { 0 } ^ { \top } \mathbf { \Sigma } _ { 0 } ^ { - 1 } \mathbf { w } \end{aligned}$
$\boldsymbol { \mu } _ { \mathbf { w } } ^ { \top } \boldsymbol { \Sigma } _ { \mathbf { w } } ^ { - 1 } = \frac { 1 } { \sigma ^ { 2 } } \mathbf { t } ^ { \top } \mathbf { X } + \boldsymbol { \mu } _ { 0 } ^ { \top } \boldsymbol { \Sigma } _ { 0 } ^ { - 1 }$
$\boldsymbol { \mu } _ { \mathrm { w } } ^ { \mathrm { T } } \boldsymbol { \Sigma } _ { \mathrm { w } } ^ { - 1 } \boldsymbol { \Sigma } _ { \mathrm { w } } = \left( \frac { 1 } { \sigma ^ { 2 } } \mathbf { t } ^ { \top } \mathbf { X } + \boldsymbol { \mu } _ { 0 } ^ { \top } \boldsymbol { \Sigma } _ { 0 } ^ { - 1 } \right) \boldsymbol { \Sigma } _ { \mathrm { w } }$
$\boldsymbol { \mu } _ { \mathbf { w } } ^ { \top } = \left( \frac { 1 } { \sigma ^ { 2 } } \mathbf { t } ^ { \top } \mathbf { X } + \boldsymbol { \mu } _ { 0 } ^ { \top } \mathbf { \Sigma } _ { 0 } ^ { - 1 } \right) \boldsymbol { \Sigma } _ { \mathbf { w } }$
$\mu _ { \mathrm { w } } = \Sigma _ { \mathrm { w } } \left( \frac { 1 } { \sigma ^ { 2 } } \mathbf { X } ^ { \mathrm { T } } \mathbf { t } + \mathbf { \Sigma } _ { 0 } ^ { - 1 } \boldsymbol { \mu } _ { 0 } \right)$
{% endkatexmm %}

```python
import numpy as np
import random
import pylab as plt

def gaussian2d(mu,sigma,xvals,yvals):
    const = (1.0/(2.0*np.pi))*(1.0/np.sqrt(np.linalg.det(sigma)))
    si = np.linalg.inv(sigma)
    xv = xvals-mu[0]
    yv = yvals-mu[1]
    return const * np.exp(-0.5*(xv*xv*si[0][0] + xv*yv*si[1][0] + yv*xv*si[0][1] + yv*yv*si[1][1]))

if __name__ == '__main__':
    data = np.loadtxt('olympic100m.txt',delimiter=',')
    x = data[:,0][:,None]
    t = data[:,1][:,None]
    x = x - x[0]
    x/= 4.0
    t = t # This is already a vector!
    inputx = x.reshape(len(x),1)
    X = np.array([[1,inputx[0]]])
    T = np.array([t[0]])
    sig = np.array([[100,0],[0,5]])
    myu = np.array([[0],[0]])
    xp = np.arange(-30,30,0.1)
    yp = np.arange(-10,10,0.1)
    Xp,Yp = np.meshgrid(xp,yp)
    Z = gaussian2d(myu,sig,Xp,Yp)
    #CS = plt.contour(Xp,Yp,Z,20,colors='k')
    #plt.xlabel('w_0')
    #plt.ylabel('w_1')
    sig_sq = 2
    plt.figure()
    plt.scatter(x,t)
    #Y = y.reshape(len(y),1)
    count = 0
    xpsub = np.arange(9,13,0.02)
    ypsub = np.arange(-0.5,0.5,0.02)
    Xpsub,Ypsub = np.meshgrid(xp,yp)
    for addx, addt in zip(inputx[1:], t[1:]):
        #print(X,T)
       count += 1
       T = np.vstack((T,addt))
       X = np.vstack((X,[1,addx]))
       #signew = np.linalg.inv((1.0/sig_sq)*np.dot(X.T,X) + np.linalg.inv(sig))
       #myunew = np.dot(signew,(1.0/sig_sq)*np.dot(X.T,T) + np.dot(np.linalg.inv(sig),myu))
       signew = np.linalg.inv((1.0/sig_sq)*X.T@X + np.linalg.inv(sig))
       myunew = signew@((1.0/sig_sq)*X.T@T + np.linalg.inv(sig)@myu)
       #print(myunew)
       if count%2== 0:
        plt.figure()
        plt.subplot(1,2,1)
        xl = np.linspace(-5,30,100)
        xline = xl.reshape(len(xl),1)
        xline = np.hstack((np.ones_like(xline),xline))
        yline = xline@myunew
        plt.plot(xl, yline)
        plt.scatter(x,t)

        plt.subplot(1,2,2)
        zposterior = gaussian2d(myunew, signew, Xpsub, Ypsub)
        cs = plt.contour(Xp, Yp,Z,20, colors = 'k')
        cs = plt.contour(Xpsub, Ypsub, zposterior,colors='r')
        plt.xlim([9,13])
        plt.ylim([-0.5,0.5])
        plt.title('First' + str(count) + ' points, contours')
    plt.show()
```
![](/assets/img/ML/bayes_0.png)


{% katexmm display %}
$cov: 
[ 1.47783251, -0.82101806 ]$
$[ - 0.82101806,  0.826491521 ]$ 
$\mu:$ $[ [ 11.57635468 ]$
$[ - 0.32019704 ] ]$

![](/assets/img/ML/bayes_1.png)

$cov: 
[ 0.6722373, - 0.12130803 ]$
$[ - 0.12130803,  0.03266264 ]$ 
$\mu:$ $[ [  11.32030208  ]$
$[ - 0.09280969 ] ]$

![](/assets/img/ML/bayes_2.png)

$cov:$
$[ 0.26539973 - 0.01359028 ]$
$[ - 0.01359028, 0.00096507 ] $


we can see that the cov is becomming more and more small with the incresing of points, that means we are less affected by the prior $\mu$ and cov.
```python
    plt.scatter(x,t)
    xnew = np.linspace(-5,50,100)
    xnew = xnew.reshape(len(xnew),1)
    xerror = np.ones_like(xnew)
    #for m in range(1,i+1):
    xerror = np.hstack((xerror,xnew))
    inverse = np.linalg.inv(X.T@X)
    newsigma = sig_sq*xerror@inverse@xerror.T
    newsigma = np.diag(newsigma)
    plt.errorbar(xnew,xerror@myunew,yerr=newsigma)
    plt.show()
```
![](/assets/img/ML/cov_error.png)
*Variance of the model*
{% endkatexmm %}</content><author><name>Linfeng Chen</name></author><category term="Tech" /><category term="Machine Learning" /><category term="Math" /><summary type="html">The likelihood p(t∣w,X,σ2)=N(Xw,σ2IN)p ( \mathbf { t } | \mathbf { w } , \mathbf { X } , \sigma ^ { 2 } ) = \mathcal { N } \left( \mathbf { X } \mathbf { w } , \sigma ^ { 2 } \mathbf { I } _ { N } \right)p(t∣w,X,σ2)=N(Xw,σ2IN​)</summary></entry><entry xml:lang="en"><title type="html">An Introduction To ESN</title><link href="http://find1dream.github.io/en/An_introduction_to_ESN" rel="alternate" type="text/html" title="An Introduction To ESN" /><published>2019-09-02T00:00:00+09:00</published><updated>2019-09-02T00:00:00+09:00</updated><id>http://find1dream.github.io/en/An_introduction_to_ESN</id><content type="html" xml:base="http://find1dream.github.io/en/An_introduction_to_ESN">![](/assets/img/RC/ESN/ESN.png)
*ESN*

## Intuitive
1. An instance of the more general concept of reservoir computing.
2. No problems of training a traditional RNN.
3. A large reservoir of sparsely connected neurons using a sigmoidal transfer function(relative to input size, like 1000 units).
4. Connections in the reservoir are assigned once and are completely random.
5. The reservoir weights do not get trained.
6. Input neurons are connected to the reservoir and feed the input activations into the reservoir - these too are assigned untrained random weights.
7. The only weights that are trained are the output weights which connect the reservoir to the output neurons.
8. Sparse random connections in the reservoir allow previous states to “echo” even after they have passed.
9. Input/output units connect to all reservoir units.
10. The output layer learns which output has to belong to a given reservoir state. - Training becomes a linear regression task.


## Model
![](/assets/img/RC/ESN/general_model.png)
*General model*

{% katexmm display %}
$$\begin{aligned}
\mathbf{r}(t+1) &amp;=f\left(W_{\text { res }}^{\text { res }} \mathbf{r}(t)+W_{\text { inp }}^{\text { res }} \mathbf{u}(t)+W_{\text { out }}^{\text { res }} \mathbf{y}(t)+W_{\text { bias }}^{\text { res }}\right) \\ \widehat{\mathbf{y}}(t+1) &amp;=W_{\text { res }}^{\text { out }} \mathbf{r}(t+1)+W_{\text { inp }}^{\text { out }} \mathbf{u}(t)+W_{\text { out }}^{\text { out }} \mathbf{y}(t)+W_{\text { bias }}^{\text { out }}
\end{aligned}
$$
If we choose $f = tanh$, we can write it like:
$$
\begin{aligned}
\mathbf{r}(t+\Delta t)=(1-\alpha) \mathbf{r}(t)+\alpha \tanh \left(\mathbf{A} \mathbf{r}(t)+\mathbf{W}_{i n} \mathbf{u}(t)+\xi 1\right)
\end{aligned}$$
Here:
* $u(t)$: input of time t.
* $\begin{aligned}W_{i np}\end{aligned}$: weights of input layer.
* $K$: num(input layer nodes)
*  $\begin{aligned}\alpha\end{aligned}$: leakage rate, control the update speed of reservoir nods.
*  $r$: reservoir state vector, recoding weight information of each reservoir node.
*  $A$: weighted adjacency matrix, usually a sparse matrix, use $\text { Erdós-Rényi }$ to generate.
*  $\begin{aligned} \mathbf{W}_{i n}\end{aligned}$: (N,M) matrix, to change M-dim signals into reservoir computable format.
*  $\begin{aligned} \mathbf{u}(t)\end{aligned}$: input signal, M-dim.
*  $\begin{aligned}\xi\end{aligned}$: bias.



![](/assets/img/RC/ESN/reservoir_computing.png)
*Running the reservoir*

Here, all weights matrices to the reservoir ($
W^{\text { res }}$) are initialized at random,
while all connections to the output ($
W^{\text { out }}
$) are trained.

## Train
### 1. Initial period
Update reservoir states $R$

```python
for t in range(trainLen):
  u = input_signal[t]
  r = (1 − alpha) ∗ r + alpha ∗ np.tanh(np.dot(A, r) + np.dot(Win, u) + bias)
  if t &gt;= initLen:
    R[:, [t − initLen]] = np.vstack((u, r))[:, 0]
```

### 2. Training peroid
After collecting all these states, use ridge regression to train parameters.
$$\begin{aligned}
W_{o u t}^{*}=\mathbf{S R}^{T}\left(\mathbf{R} \mathbf{R}^{T}+\beta \mathbf{I}\right)^{-1}
\end{aligned}$$
$S$ means target matrix

## Test
To test reservoir:
$$\begin{aligned}
\hat{\mathbf{s}}=\mathbf{W}_{o u t} \mathbf{r}(t)
\end{aligned}$$
```python
S = np.zeros((P, testLen))
u = input_signal[trainLen]
for t in range(testLen):
  r = (1 − alpha) ∗ r + alpha ∗ np.tanh(np.dot(A, r) + np.dot(Win, u) + bias) 
  s = np.dot(Wout, np.vstack((u, r)))
  S[:, t] = s
  u=s
```

## Evaluation
RMS(root mean square) error.


## Others
### Echo state property
Input now has bigger effect on inner states than previous input or states.\\
That means it has to dismiss initial states when stable.\\
To meet ESP, the biggest eigenvalue of $W^{\text { res }} $ has to closer to 1 and less than 1.\\
(in program, make the max eigenvalue scales to 0.99 is ok)
## Reference
1. [An overview of reservoir computing: theory, applications and implementations](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2007-8.pdf)
2. [Introduction to reservoir computing](http://jujuba.me/articles/reservoir_computing.html)
3. [ゼロから作るReservoir Computing](https://qiita.com/pokotsun/items/dd8eb48fadeee052110b)
4. [Echo state network](http://www.scholarpedia.org/article/Echo_state_network)















{% endkatexmm %}</content><author><name>Linfeng Chen</name></author><category term="Tech" /><category term="Reservoir Computing" /><category term="Math" /><summary type="html">ESN</summary></entry><entry xml:lang="en"><title type="html">Modeling With NMF And SVD</title><link href="http://find1dream.github.io/en/Modeling_with_NMF_and_SVD" rel="alternate" type="text/html" title="Modeling With NMF And SVD" /><published>2019-09-01T00:00:00+09:00</published><updated>2019-09-01T00:00:00+09:00</updated><id>http://find1dream.github.io/en/Modeling_with_NMF_and_SVD</id><content type="html" xml:base="http://find1dream.github.io/en/Modeling_with_NMF_and_SVD">### Why we decomposing a matrix:

Decomposing matrices into matrices that have special properties.

### Where we can use it?
* semantic analysis
* collaborative filtering/recommendations ([winning entry for Netflix Prize](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf))
* calculate Moore-Penrose pseudoinverse
* data compression
* [principal component analysis](https://find1dream.github.io/en/PCA_and_SVD)

### How we can decompose a matrix
* SVD
* NMF

## SVD
Eg: image composition\\
SVD is an exact decomposition, since the matrices it creates are big enough to fully cover the original matrix(you can recover your matrix).

{% katexmm display %}
### Singular value:
Can give us kind of measure of importance of matrix

![](/assets/img/ML/PCA_etc/ranking.png)
*Ranking*




### How to separate different topics?
We would clearly expect that the words that appear most frequently in one topic would appear less frequently in the other - otherwise that word wouldn’t make a good choice to separate out the two topics. Therefore, we expect the topics to be **[orthogonal](https://en.wikipedia.org/wiki/Orthogonal_matrix)**(eg: columns of $U$ or rows of $V$ are orthogonal).

![](/assets/img/ML/PCA_etc/UAV.png)


### What the matrix means in a intuitive way?
$U$ row direction means the first column of matrix $A$, and columns means topics, they are orthogonal.\\
$S$ is diagonal matrix and ordered in descending order, give us a sense of importance.(topics by topics)\\
$V$ row direction means topics, and column direction means different words frequencies.

```python
#how to show that the result of linalg.svd is a decomposition of the input
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn import decomposition
from scipy import linalg
import matplotlib.pyplot as plt
categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']
remove = ('headers', 'footers', 'quotes')
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)
newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)
num_topics, num_top_words = 6, 8
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
vectorizer = CountVectorizer(stop_words='english')
vectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab)
vectors.shape #, vectors.nnz / vectors.shape[0], row_means.shape
vocab = np.array(vectorizer.get_feature_names())
%time U, s, Vh = linalg.svd(vectors, full_matrices=False)
n = U @ np.diag(s) @ Vh
np.linalg.norm(n-vectors)  ## which is very small
```

```python
## how to show that U and V are orthonormal
np.allclose (U @ U.T, np.eye(U.shape[0]))
np.allclose (Vh @ Vh.T, np.eye(U.shape[0]))
```

```python
# show topics that are biggest
num_top_words=8

def show_topics(a):
    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]
    topic_words = ([top_words(t) for t in a])
    return [' '.join(t) for t in topic_words]
show_topics(Vh[:10])
```

## NMF
Rather than constraining our factors to be orthogonal, another idea would to constrain them to be non-negative. NMF is a factorization of a non-negative data set $V$: $$ V = W H$$ into non-negative matrices $W,\; H$. Often positive factors will be more easily interpretable (and this is the reason behind NMF’s popularity).

Benefits: Fast and easy to use!\\
Downsides: Took years of research and expertise to create

### Applications of NMF
* Face Decompositions
* Collaborative Filtering, eg movie recommendations
* Audio source separation
* Chemistry
* Bioinformatics and Gene Expression
* Topic Modeling (our problem!)
![](/assets/img/ML/PCA_etc/App_NMF.png)


### SGD
Applying SGD to NMF\\
Goal: Decompose $V\;(m \times n)$ into $$V \approx WH$$ where $W\;(m \times d)$ and $H\;(d \times n)$, $W,H &gt; 0$, and we’ve minimized the Frobenius norm of $V-WH$.\\
Approach: We will pick random positive $W$ &amp; $H$, and then use SGD to optimize.\\
To use SGD, we need to know the gradient of the loss function.
{% endkatexmm %}</content><author><name>Linfeng Chen</name></author><category term="Tech" /><category term="Machine Learning" /><category term="Math" /><summary type="html">Why we decomposing a matrix:</summary></entry><entry xml:lang="en"><title type="html">Interesting Papers To Replicate</title><link href="http://find1dream.github.io/en/Interesting_papers_to_replicate" rel="alternate" type="text/html" title="Interesting Papers To Replicate" /><published>2019-08-29T00:00:00+09:00</published><updated>2019-08-29T00:00:00+09:00</updated><id>http://find1dream.github.io/en/Interesting_papers_to_replicate</id><content type="html" xml:base="http://find1dream.github.io/en/Interesting_papers_to_replicate"># Paper to replicate
Here is the list of mostly CV papers you should implement one after another.
Author of this list claims that after one of his interns implemented this list he became junior researcher from a student without prior knowledge in ML in one month
Architectures

* [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
* [ZFNet](https://arxiv.org/abs/1311.2901)
* [VGG16](https://arxiv.org/pdf/1409.1556)
* [ResNet](https://arxiv.org/abs/1704.06904)
* [GoogLeNet](https://arxiv.org/abs/1409.4842)
* [Inception](https://arxiv.org/abs/1512.00567)
* [Xception](https://arxiv.org/abs/1610.02357)
* [MobileNet](https://arxiv.org/abs/1704.04861)


Semantic Segmentation

* [FCN](https://arxiv.org/abs/1411.4038)
* [SegNet](https://arxiv.org/abs/1511.00561)
* [UNet](https://arxiv.org/abs/1505.04597)
* [PSPNet](https://arxiv.org/abs/1612.01105)
* [DeepLab](https://arxiv.org/abs/1606.00915)
* [ICNet](https://arxiv.org/abs/1704.08545)
* [ENet](https://arxiv.org/abs/1606.02147)


Generative adversarial networks

* [GAN](https://arxiv.org/abs/1406.2661)
* [DCGAN](https://arxiv.org/abs/1511.06434)
* [WGAN](https://arxiv.org/abs/1701.07875)
* [Pix2Pix](https://arxiv.org/abs/1611.07004)
* [CycleGAN](https://arxiv.org/abs/1703.10593)


Object detection

* [RCNN: https](//arxiv.org/abs/1311.2524)
* [Fast-RCNN](https://arxiv.org/abs/1504.08083)
* [Faster-RCNN](https://arxiv.org/abs/1506.01497)
* [SSD](https://arxiv.org/abs/1512.02325)
* [YOLO](https://arxiv.org/abs/1506.02640)
* [YOLO9000](https://arxiv.org/abs/1612.08242)</content><author><name>Linfeng Chen</name></author><category term="Share" /><category term="Deep Learning" /><summary type="html">Paper to replicate Here is the list of mostly CV papers you should implement one after another. Author of this list claims that after one of his interns implemented this list he became junior researcher from a student without prior knowledge in ML in one month Architectures</summary></entry><entry xml:lang="en"><title type="html">Feature Engineering For Machine Learning</title><link href="http://find1dream.github.io/en/Feature_engineering" rel="alternate" type="text/html" title="Feature Engineering For Machine Learning" /><published>2019-08-28T00:00:00+09:00</published><updated>2019-08-28T00:00:00+09:00</updated><id>http://find1dream.github.io/en/Feature_engineering</id><content type="html" xml:base="http://find1dream.github.io/en/Feature_engineering">## What is feature engineering?
Feature engineering is the act of extracting features from raw data and transforming them into formats that are suitable for the machine learn‐ ing model.

If there are too many features, or if most of them are irrelevant, then the model will be more expen‐ sive and tricky to train.

## What means good features?
Good features should not only represent salient aspects of the data, but also conform to the assumptions of the model.

{% katexmm display %}
$\rightarrow$ Transformations are often necessary.

## How to check?
1. whether the magnitude matters
2. consider the scale of the features
   1. min-max scaling
   2. standardization(but don’t center sparse data, computational burden)
   3. $\ell^{2}$ normalization
3. consider the distribution of numeric features. linear function assumes the gaussian distribution(use log transforms)
4. data integration



{% endkatexmm %}

## Feature engineering link


### Theory
1. [Feature Engineering](https://www.slideshare.net/HJvanVeen/feature-engineering-72376750)
2. [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
3. [カテゴリカル変数のEncoding手法のまとめ](http://jotkn.ciao.jp/wp/2017/08/22/post-67/)
4. [Feature Engineering: Data scientist’s Secret Sauce !](https://www.linkedin.com/pulse/feature-engineering-data-scientists-secret-sauce-ashish-kumar?trk=prof-post)


### Practice
1. [DataFrameで特徴量作るのめんどくさ過ぎる。。featuretoolsを使って自動生成したろ](https://qiita.com/Hyperion13fleet/items/4eaca365f28049fe11c7)
2. [Automated Feature Engineering Basics](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)
3. [Introduction to Manual Feature Engineering](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
4. [Introduction to Manual Feature Engineering P2](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)
5. [KaggleのWinner solutionにもなった「K近傍を用いた特徴量抽出」のPython実装](https://upura.hatenablog.com/entry/2018/06/23/165855)
6. [カテゴリカル変数のEncoding手法について](http://nami3373.hatenablog.com/entry/2018/07/26/230655)
7. [fast_feng](https://github.com/tomomotofactory/ffeng)
8. [学習アルゴリズム以外のscikit-learn便利機能と連携ライブラリ](https://yubessy.hatenablog.com/entry/2016/02/17/164511)
9. [遺伝的プログラミングによる特徴量生成](https://qiita.com/overlap/items/e7f1077ef8239f454602)
10. [遺伝的プログラミングによる特徴量生成でLightGBMの精度向上](https://upura.hatenablog.com/entry/2018/12/11/000000)

### Course
1. [How to Win a Data Science Competition: Learn from Top Kagglers](https://www.coursera.org/learn/competitive-data-science)

### Book
1. [Feature Engineering for Machine Learning](https://www.amazon.co.jp/Feature-Engineering-Machine-Learning-Principles/dp/1491953241/ref=as_sl_pc_qf_sp_asin_til?tag=hiroshi128-22&amp;linkCode=w00&amp;linkId=354af39de1f0c6a0c7da56ca1a647509&amp;creativeASIN=1491953241)</content><author><name>Linfeng Chen</name></author><category term="Share" /><category term="Feature Engineering" /><category term="Machine Learning" /><summary type="html">What is feature engineering? Feature engineering is the act of extracting features from raw data and transforming them into formats that are suitable for the machine learn‐ ing model.</summary></entry></feed>